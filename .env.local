CORE_HOST=beefy.ip4
# address and port from which to accept incoming connections
LLAMA_HOST=0.0.0.0
LLAMA_PORT=8000

# Folder containing the models. This is were Docker mounts a volume
# Mandatory!
MODELS_FOLDER=models

# Name of the model file. This should be inside MODELS_FOLDER
# Mandatory!
MODEL=openhermes-2.5-mistral-7b.Q6_K.gguf

# Length in tokens of the model's context
N_CTX=16384

# Number of model's layer to off-load on the GPU
N_GPU_LAYERS=0

# Index of the main GPU
# MAIN_GPU=0

# Number of CPU threads
N_THREADS=0

# Number of token to consider for repetition penalty
# LAST_N_TOKENS_SIZE=64

# Format of the conversation to use
# CHAT_FORMAT=llama-2

# Whether to interrupt requests when a new request is received
# INTERRUPT_REQUESTS=True

# Verbose=True
